{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7615428,"sourceType":"datasetVersion","datasetId":4434986}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T16:42:14.010507Z","iopub.execute_input":"2025-03-23T16:42:14.010747Z","iopub.status.idle":"2025-03-23T16:42:16.085181Z","shell.execute_reply.started":"2025-03-23T16:42:14.010723Z","shell.execute_reply":"2025-03-23T16:42:16.084086Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/faceforensics/FF++/fake/02_13__exit_phone_room__CP5HFV3K.mp4\n/kaggle/input/faceforensics/FF++/fake/03_14__talking_against_wall__ZC2KYASW.mp4\n/kaggle/input/faceforensics/FF++/fake/03_15__outside_talking_pan_laughing__Y11NT1YX.mp4\n/kaggle/input/faceforensics/FF++/fake/07_26__walking_down_street_outside_angry__FGNGC2GT.mp4\n/kaggle/input/faceforensics/FF++/fake/07_03__hugging_happy__7NGMD8FT.mp4\n/kaggle/input/faceforensics/FF++/fake/07_09__walk_down_hall_angry__N9CWME71.mp4\n/kaggle/input/faceforensics/FF++/fake/01_12__outside_talking_pan_laughing__TNI7KUZ6.mp4\n/kaggle/input/faceforensics/FF++/fake/07_03__podium_speech_happy__6PHZRQ4H.mp4\n/kaggle/input/faceforensics/FF++/fake/07_02__walking_down_street_outside_angry__O4SXNLRL.mp4\n/kaggle/input/faceforensics/FF++/fake/08_05__walk_down_hall_angry__FBICSP2C.mp4\n/kaggle/input/faceforensics/FF++/fake/06_18__outside_talking_pan_laughing__DEA1TCLN.mp4\n/kaggle/input/faceforensics/FF++/fake/01_03__talking_against_wall__JZUXXFRB.mp4\n/kaggle/input/faceforensics/FF++/fake/02_06__podium_speech_happy__N8OSN8P6.mp4\n/kaggle/input/faceforensics/FF++/fake/09_07__kitchen_pan__N9CWME71.mp4\n/kaggle/input/faceforensics/FF++/fake/01_02__walk_down_hall_angry__YVGY8LOK.mp4\n/kaggle/input/faceforensics/FF++/fake/02_06__talking_angry_couch__MKZTXQ2T.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__talking_angry_couch__I8G2LWD1.mp4\n/kaggle/input/faceforensics/FF++/fake/02_13__secret_conversation__CP5HFV3K.mp4\n/kaggle/input/faceforensics/FF++/fake/02_03__walking_down_street_outside_angry__QH3Y0IG0.mp4\n/kaggle/input/faceforensics/FF++/fake/04_01__outside_talking_still_laughing__6I623VU9.mp4\n/kaggle/input/faceforensics/FF++/fake/03_07__hugging_happy__BKLOCI1M.mp4\n/kaggle/input/faceforensics/FF++/fake/05_08__talking_against_wall__PRBCE28Z.mp4\n/kaggle/input/faceforensics/FF++/fake/06_15__outside_talking_still_laughing__QRCD27P8.mp4\n/kaggle/input/faceforensics/FF++/fake/07_03__secret_conversation__IFSURI9X.mp4\n/kaggle/input/faceforensics/FF++/fake/02_07__walk_down_hall_angry__U7DEOZNV.mp4\n/kaggle/input/faceforensics/FF++/fake/02_06__walking_and_outside_surprised__N8OSN8P6.mp4\n/kaggle/input/faceforensics/FF++/fake/03_13__kitchen_pan__GBYWJW06.mp4\n/kaggle/input/faceforensics/FF++/fake/03_21__secret_conversation__YCSEBZO4.mp4\n/kaggle/input/faceforensics/FF++/fake/07_02__talking_angry_couch__1H07DFQJ.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__walking_and_outside_surprised__HTG660F8.mp4\n/kaggle/input/faceforensics/FF++/fake/03_15__kitchen_pan__AIOM1U5V.mp4\n/kaggle/input/faceforensics/FF++/fake/03_07__walk_down_hall_angry__IFSURI9X.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__secret_conversation__MZWH8ATN.mp4\n/kaggle/input/faceforensics/FF++/fake/03_07__walking_outside_cafe_disgusted__IFSURI9X.mp4\n/kaggle/input/faceforensics/FF++/fake/03_14__walk_down_hall_angry__P1L5PF4I.mp4\n/kaggle/input/faceforensics/FF++/fake/01_11__talking_against_wall__9229VVZ3.mp4\n/kaggle/input/faceforensics/FF++/fake/01_11__walking_outside_cafe_disgusted__FAFWDR4W.mp4\n/kaggle/input/faceforensics/FF++/fake/06_18__podium_speech_happy__DEA1TCLN.mp4\n/kaggle/input/faceforensics/FF++/fake/07_20__outside_talking_pan_laughing__KV6Q7D6C.mp4\n/kaggle/input/faceforensics/FF++/fake/06_04__walking_outside_cafe_disgusted__ZK95PQDE.mp4\n/kaggle/input/faceforensics/FF++/fake/04_21__walking_down_street_outside_angry__5Y31RZP8.mp4\n/kaggle/input/faceforensics/FF++/fake/07_03__walking_outside_cafe_disgusted__F0YYEA5W.mp4\n/kaggle/input/faceforensics/FF++/fake/09_02__walking_down_street_outside_angry__6KUOFMZW.mp4\n/kaggle/input/faceforensics/FF++/fake/10_19__kitchen_still__IDX76N5R.mp4\n/kaggle/input/faceforensics/FF++/fake/02_13__outside_talking_pan_laughing__2YSYT2N3.mp4\n/kaggle/input/faceforensics/FF++/fake/04_06__kitchen_still__ZK95PQDE.mp4\n/kaggle/input/faceforensics/FF++/fake/06_02__podium_speech_happy__N8OSN8P6.mp4\n/kaggle/input/faceforensics/FF++/fake/03_27__walk_down_hall_angry__IL675GCI.mp4\n/kaggle/input/faceforensics/FF++/fake/04_13__walking_outside_cafe_disgusted__00T3UYOR.mp4\n/kaggle/input/faceforensics/FF++/fake/02_07__walking_and_outside_surprised__1VMZUH1W.mp4\n/kaggle/input/faceforensics/FF++/fake/01_03__hugging_happy__ISF9SP4G.mp4\n/kaggle/input/faceforensics/FF++/fake/09_18__outside_talking_pan_laughing__3VP8836C.mp4\n/kaggle/input/faceforensics/FF++/fake/07_06__outside_talking_still_laughing__NMGYPBXE.mp4\n/kaggle/input/faceforensics/FF++/fake/03_27__walking_down_indoor_hall_disgust__IL675GCI.mp4\n/kaggle/input/faceforensics/FF++/fake/03_13__meeting_serious__T3MZOI8X.mp4\n/kaggle/input/faceforensics/FF++/fake/07_14__talking_against_wall__P9QFO50U.mp4\n/kaggle/input/faceforensics/FF++/fake/06_14__walking_and_outside_surprised__8U9ULZDT.mp4\n/kaggle/input/faceforensics/FF++/fake/09_26__talking_against_wall__C3K20JOL.mp4\n/kaggle/input/faceforensics/FF++/fake/02_14__outside_talking_pan_laughing__3IUBEKCT.mp4\n/kaggle/input/faceforensics/FF++/fake/06_12__outside_talking_pan_laughing__3K21NFNM.mp4\n/kaggle/input/faceforensics/FF++/fake/05_16__walk_down_hall_angry__U9WZI5LK.mp4\n/kaggle/input/faceforensics/FF++/fake/03_07__walking_outside_cafe_disgusted__CDSNLDQ8.mp4\n/kaggle/input/faceforensics/FF++/fake/02_27__hugging_happy__GVFLSZD5.mp4\n/kaggle/input/faceforensics/FF++/fake/06_07__kitchen_pan__NMGYPBXE.mp4\n/kaggle/input/faceforensics/FF++/fake/06_25__outside_talking_pan_laughing__MI9BDQ7M.mp4\n/kaggle/input/faceforensics/FF++/fake/09_01__talking_against_wall__O8HNNX43.mp4\n/kaggle/input/faceforensics/FF++/fake/06_14__outside_talking_pan_laughing__8U9ULZDT.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__outside_talking_pan_laughing__I8G2LWD1.mp4\n/kaggle/input/faceforensics/FF++/fake/07_25__walk_down_hall_angry__PAE9HCA8.mp4\n/kaggle/input/faceforensics/FF++/fake/04_13__podium_speech_happy__00T3UYOR.mp4\n/kaggle/input/faceforensics/FF++/fake/01_21__walk_down_hall_angry__03X7CELV.mp4\n/kaggle/input/faceforensics/FF++/fake/03_11__exit_phone_room__P08VGHTA.mp4\n/kaggle/input/faceforensics/FF++/fake/01_27__outside_talking_still_laughing__ZYCZ30C0.mp4\n/kaggle/input/faceforensics/FF++/fake/06_04__outside_talking_still_laughing__ZK95PQDE.mp4\n/kaggle/input/faceforensics/FF++/fake/09_20__podium_speech_happy__O5X0AWR9.mp4\n/kaggle/input/faceforensics/FF++/fake/03_06__meeting_serious__83ABVHC3.mp4\n/kaggle/input/faceforensics/FF++/fake/05_17__hugging_happy__YTJYYDO9.mp4\n/kaggle/input/faceforensics/FF++/fake/03_07__walking_outside_cafe_disgusted__PWXXULHR.mp4\n/kaggle/input/faceforensics/FF++/fake/02_03__walking_outside_cafe_disgusted__QH3Y0IG0.mp4\n/kaggle/input/faceforensics/FF++/fake/07_06__walking_down_street_outside_angry__NMGYPBXE.mp4\n/kaggle/input/faceforensics/FF++/fake/09_25__walking_down_street_outside_angry__5041ODBN.mp4\n/kaggle/input/faceforensics/FF++/fake/07_12__outside_talking_still_laughing__0VN0A2T3.mp4\n/kaggle/input/faceforensics/FF++/fake/03_06__podium_speech_happy__83ABVHC3.mp4\n/kaggle/input/faceforensics/FF++/fake/02_09__kitchen_pan__HIH8YA82.mp4\n/kaggle/input/faceforensics/FF++/fake/07_03__talking_angry_couch__WPT3Z2KN.mp4\n/kaggle/input/faceforensics/FF++/fake/09_20__talking_angry_couch__0IRM5ADD.mp4\n/kaggle/input/faceforensics/FF++/fake/05_16__exit_phone_room__B62WCGUN.mp4\n/kaggle/input/faceforensics/FF++/fake/06_12__podium_speech_happy__3K21NFNM.mp4\n/kaggle/input/faceforensics/FF++/fake/01_11__meeting_serious__9OM3VE0Y.mp4\n/kaggle/input/faceforensics/FF++/fake/02_27__walk_down_hall_angry__78M8S6M6.mp4\n/kaggle/input/faceforensics/FF++/fake/05_16__walk_down_hall_angry__OSXCUOHX.mp4\n/kaggle/input/faceforensics/FF++/fake/04_26__outside_talking_still_laughing__WX836VLY.mp4\n/kaggle/input/faceforensics/FF++/fake/02_07__walking_down_street_outside_angry__O4SXNLRL.mp4\n/kaggle/input/faceforensics/FF++/fake/04_07__exit_phone_room__ITC0C48B.mp4\n/kaggle/input/faceforensics/FF++/fake/09_02__walk_down_hall_angry__6KUOFMZW.mp4\n/kaggle/input/faceforensics/FF++/fake/06_18__walk_down_hall_angry__LH0KWJKM.mp4\n/kaggle/input/faceforensics/FF++/fake/03_26__kitchen_pan__WBSXBL82.mp4\n/kaggle/input/faceforensics/FF++/fake/04_13__secret_conversation__00T3UYOR.mp4\n/kaggle/input/faceforensics/FF++/fake/01_20__outside_talking_pan_laughing__OTGHOG4Z.mp4\n/kaggle/input/faceforensics/FF++/fake/03_18__walking_outside_cafe_disgusted__22UBC0BS.mp4\n/kaggle/input/faceforensics/FF++/fake/07_27__walking_down_street_outside_angry__3RH5PR6S.mp4\n/kaggle/input/faceforensics/FF++/fake/03_15__outside_talking_still_laughing__DNUJD8M2.mp4\n/kaggle/input/faceforensics/FF++/fake/02_13__podium_speech_happy__2YSYT2N3.mp4\n/kaggle/input/faceforensics/FF++/fake/02_18__exit_phone_room__OXMEEFUQ.mp4\n/kaggle/input/faceforensics/FF++/fake/09_01__walk_down_hall_angry__6TSGVLHA.mp4\n/kaggle/input/faceforensics/FF++/fake/06_14__walking_down_indoor_hall_disgust__8U9ULZDT.mp4\n/kaggle/input/faceforensics/FF++/fake/01_02__outside_talking_still_laughing__YVGY8LOK.mp4\n/kaggle/input/faceforensics/FF++/fake/07_21__outside_talking_still_laughing__K7KXUHMU.mp4\n/kaggle/input/faceforensics/FF++/fake/02_12__podium_speech_happy__9D2ZHEKW.mp4\n/kaggle/input/faceforensics/FF++/fake/06_07__exit_phone_room__NMGYPBXE.mp4\n/kaggle/input/faceforensics/FF++/fake/06_03__talking_against_wall__4I8LRXWF.mp4\n/kaggle/input/faceforensics/FF++/fake/03_26__talking_against_wall__WBSXBL82.mp4\n/kaggle/input/faceforensics/FF++/fake/01_11__secret_conversation__4OJNJLOO.mp4\n/kaggle/input/faceforensics/FF++/fake/01_20__outside_talking_still_laughing__FW94AIMJ.mp4\n/kaggle/input/faceforensics/FF++/fake/06_25__walking_and_outside_surprised__MI9BDQ7M.mp4\n/kaggle/input/faceforensics/FF++/fake/04_18__kitchen_still__NAXINA1N.mp4\n/kaggle/input/faceforensics/FF++/fake/02_18__outside_talking_pan_laughing__OXMEEFUQ.mp4\n/kaggle/input/faceforensics/FF++/fake/06_15__exit_phone_room__QRCD27P8.mp4\n/kaggle/input/faceforensics/FF++/fake/07_20__talking_against_wall__KV6Q7D6C.mp4\n/kaggle/input/faceforensics/FF++/fake/03_07__walking_down_indoor_hall_disgust__PWXXULHR.mp4\n/kaggle/input/faceforensics/FF++/fake/07_13__walking_outside_cafe_disgusted__RVQCPCJF.mp4\n/kaggle/input/faceforensics/FF++/fake/01_27__hugging_happy__ZYCZ30C0.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__exit_phone_room__MZWH8ATN.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__walk_down_hall_angry__TN2CWM3K.mp4\n/kaggle/input/faceforensics/FF++/fake/04_01__exit_phone_room__0XUW13RW.mp4\n/kaggle/input/faceforensics/FF++/fake/06_26__kitchen_still__L5BVR5L9.mp4\n/kaggle/input/faceforensics/FF++/fake/03_11__talking_against_wall__P08VGHTA.mp4\n/kaggle/input/faceforensics/FF++/fake/02_07__meeting_serious__1JCLEEBQ.mp4\n/kaggle/input/faceforensics/FF++/fake/03_21__meeting_serious__V53E3RVB.mp4\n/kaggle/input/faceforensics/FF++/fake/09_13__kitchen_pan__21H6XSPE.mp4\n/kaggle/input/faceforensics/FF++/fake/04_01__outside_talking_still_laughing__0XUW13RW.mp4\n/kaggle/input/faceforensics/FF++/fake/06_02__walk_down_hall_angry__37DH75GQ.mp4\n/kaggle/input/faceforensics/FF++/fake/02_21__kitchen_pan__Z0XHPQAR.mp4\n/kaggle/input/faceforensics/FF++/fake/03_04__outside_talking_pan_laughing__T04P6ELC.mp4\n/kaggle/input/faceforensics/FF++/fake/04_01__secret_conversation__6I623VU9.mp4\n/kaggle/input/faceforensics/FF++/fake/03_15__podium_speech_happy__DG8ITQO3.mp4\n/kaggle/input/faceforensics/FF++/fake/03_13__outside_talking_still_laughing__GBYWJW06.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__exit_phone_room__I8G2LWD1.mp4\n/kaggle/input/faceforensics/FF++/fake/05_28__exit_phone_room__U9LRLJ6N.mp4\n/kaggle/input/faceforensics/FF++/fake/03_18__walking_and_outside_surprised__8HNSXOBW.mp4\n/kaggle/input/faceforensics/FF++/fake/03_15__kitchen_pan__DNUJD8M2.mp4\n/kaggle/input/faceforensics/FF++/fake/07_09__outside_talking_still_laughing__N9CWME71.mp4\n/kaggle/input/faceforensics/FF++/fake/06_18__outside_talking_still_laughing__M36D0OJT.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__talking_against_wall__HTG660F8.mp4\n/kaggle/input/faceforensics/FF++/fake/09_03__kitchen_pan__8DTEGQ54.mp4\n/kaggle/input/faceforensics/FF++/fake/10_22__kitchen_still__EHARPYBT.mp4\n/kaggle/input/faceforensics/FF++/fake/08_28__outside_talking_pan_laughing__8BC35RFU.mp4\n/kaggle/input/faceforensics/FF++/fake/02_21__talking_angry_couch__Z0XHPQAR.mp4\n/kaggle/input/faceforensics/FF++/fake/03_01__walking_and_outside_surprised__JZUXXFRB.mp4\n/kaggle/input/faceforensics/FF++/fake/06_20__kitchen_pan__6SUW7063.mp4\n/kaggle/input/faceforensics/FF++/fake/06_07__walking_down_street_outside_angry__NMGYPBXE.mp4\n/kaggle/input/faceforensics/FF++/fake/06_25__talking_angry_couch__MI9BDQ7M.mp4\n/kaggle/input/faceforensics/FF++/fake/02_01__secret_conversation__YVGY8LOK.mp4\n/kaggle/input/faceforensics/FF++/fake/03_04__talking_angry_couch__T04P6ELC.mp4\n/kaggle/input/faceforensics/FF++/fake/02_18__walking_down_street_outside_angry__21JTDDEL.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__walking_and_outside_surprised__I8G2LWD1.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__outside_talking_pan_laughing__SB6PMCO0.mp4\n/kaggle/input/faceforensics/FF++/fake/06_11__walking_outside_cafe_disgusted__MX659QU8.mp4\n/kaggle/input/faceforensics/FF++/fake/09_02__walking_down_street_outside_angry__9TDCEK1Q.mp4\n/kaggle/input/faceforensics/FF++/fake/07_15__hugging_happy__9Z2MLKVX.mp4\n/kaggle/input/faceforensics/FF++/fake/03_14__walking_down_street_outside_angry__H0VQHGS3.mp4\n/kaggle/input/faceforensics/FF++/fake/03_09__outside_talking_still_laughing__RCETIXYL.mp4\n/kaggle/input/faceforensics/FF++/fake/09_20__kitchen_pan__98NUQ3E6.mp4\n/kaggle/input/faceforensics/FF++/fake/02_06__talking_angry_couch__GH8TGTBS.mp4\n/kaggle/input/faceforensics/FF++/fake/03_09__secret_conversation__RCETIXYL.mp4\n/kaggle/input/faceforensics/FF++/fake/03_21__exit_phone_room__YCSEBZO4.mp4\n/kaggle/input/faceforensics/FF++/fake/01_27__walking_outside_cafe_disgusted__ZYCZ30C0.mp4\n/kaggle/input/faceforensics/FF++/fake/07_26__walk_down_hall_angry__FGNGC2GT.mp4\n/kaggle/input/faceforensics/FF++/fake/04_01__kitchen_still__6I623VU9.mp4\n/kaggle/input/faceforensics/FF++/fake/09_13__outside_talking_pan_laughing__LPT427RY.mp4\n/kaggle/input/faceforensics/FF++/fake/03_14__podium_speech_happy__Q9NSXM88.mp4\n/kaggle/input/faceforensics/FF++/fake/06_27__walking_and_outside_surprised__O7L5Z9U8.mp4\n/kaggle/input/faceforensics/FF++/fake/06_12__podium_speech_happy__0VR4Y891.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__talking_angry_couch__HTG660F8.mp4\n/kaggle/input/faceforensics/FF++/fake/05_28__kitchen_still__W3J028UG.mp4\n/kaggle/input/faceforensics/FF++/fake/09_26__walk_down_hall_angry__QSE5A0GF.mp4\n/kaggle/input/faceforensics/FF++/fake/10_22__kitchen_pan__EHARPYBT.mp4\n/kaggle/input/faceforensics/FF++/fake/07_21__talking_against_wall__MKU99DVX.mp4\n/kaggle/input/faceforensics/FF++/fake/07_14__exit_phone_room__P9QFO50U.mp4\n/kaggle/input/faceforensics/FF++/fake/06_26__kitchen_pan__L5BVR5L9.mp4\n/kaggle/input/faceforensics/FF++/fake/02_25__talking_against_wall__Z7FQ69VP.mp4\n/kaggle/input/faceforensics/FF++/fake/04_13__walking_down_street_outside_angry__00T3UYOR.mp4\n/kaggle/input/faceforensics/FF++/fake/04_12__walking_down_street_outside_angry__96TQKDFJ.mp4\n/kaggle/input/faceforensics/FF++/fake/07_21__exit_phone_room__K7KXUHMU.mp4\n/kaggle/input/faceforensics/FF++/fake/07_06__kitchen_pan__NMGYPBXE.mp4\n/kaggle/input/faceforensics/FF++/fake/03_07__walk_down_hall_angry__6PHZRQ4H.mp4\n/kaggle/input/faceforensics/FF++/fake/03_21__kitchen_pan__YCSEBZO4.mp4\n/kaggle/input/faceforensics/FF++/fake/06_15__walking_and_outside_surprised__QRCD27P8.mp4\n/kaggle/input/faceforensics/FF++/fake/07_03__outside_talking_pan_laughing__IFSURI9X.mp4\n/kaggle/input/faceforensics/FF++/fake/04_07__kitchen_pan__XRK7FGZX.mp4\n/kaggle/input/faceforensics/FF++/fake/01_03__podium_speech_happy__480LQD1C.mp4\n/kaggle/input/faceforensics/FF++/fake/08_16__podium_speech_happy__8Q7JCS95.mp4\n/kaggle/input/faceforensics/FF++/fake/05_17__walking_down_street_outside_angry__M3H96PDQ.mp4\n/kaggle/input/faceforensics/FF++/fake/09_03__talking_against_wall__8DTEGQ54.mp4\n/kaggle/input/faceforensics/FF++/fake/02_15__walking_and_outside_surprised__MZWH8ATN.mp4\n/kaggle/input/faceforensics/FF++/fake/03_26__kitchen_still__WBSXBL82.mp4\n/kaggle/input/faceforensics/FF++/fake/02_06__walking_down_indoor_hall_disgust__U6MDWIHG.mp4\n/kaggle/input/faceforensics/FF++/fake/06_27__talking_angry_couch__JOG5PB18.mp4\n/kaggle/input/faceforensics/FF++/fake/03_13__meeting_serious__GBYWJW06.mp4\n/kaggle/input/faceforensics/FF++/fake/02_09__exit_phone_room__HIH8YA82.mp4\n/kaggle/input/faceforensics/FF++/real/08__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/14__walking_down_indoor_hall_disgust.mp4\n/kaggle/input/faceforensics/FF++/real/08__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/05__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/14__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/06__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/12__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/02__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/01__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/03__hugging_happy.mp4\n/kaggle/input/faceforensics/FF++/real/12__walking_down_indoor_hall_disgust.mp4\n/kaggle/input/faceforensics/FF++/real/05__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/12__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/08__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/03__secret_conversation.mp4\n/kaggle/input/faceforensics/FF++/real/16__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/11__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/09__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/13__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/14__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/07__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/04__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/14__secret_conversation.mp4\n/kaggle/input/faceforensics/FF++/real/06__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/14__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/07__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/14__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/06__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/15__walking_and_outside_surprised.mp4\n/kaggle/input/faceforensics/FF++/real/14__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/07__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/09__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/06__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/07__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/09__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/01__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/13__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/12__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/04__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/01__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/02__walking_and_outside_surprised.mp4\n/kaggle/input/faceforensics/FF++/real/15__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/12__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/12__walking_and_outside_surprised.mp4\n/kaggle/input/faceforensics/FF++/real/07__hugging_happy.mp4\n/kaggle/input/faceforensics/FF++/real/09__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/06__hugging_happy.mp4\n/kaggle/input/faceforensics/FF++/real/11__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/10__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/10__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/14__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/13__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/02__walking_down_indoor_hall_disgust.mp4\n/kaggle/input/faceforensics/FF++/real/13__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/13__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/07__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/07__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/06__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/11__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/03__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/09__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/12__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/06__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/06__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/13__walking_down_indoor_hall_disgust.mp4\n/kaggle/input/faceforensics/FF++/real/03__walking_down_indoor_hall_disgust.mp4\n/kaggle/input/faceforensics/FF++/real/11__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/05__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/12__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/12__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/11__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/08__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/01__walking_and_outside_surprised.mp4\n/kaggle/input/faceforensics/FF++/real/14__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/13__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/08__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/04__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/02__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/04__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/11__secret_conversation.mp4\n/kaggle/input/faceforensics/FF++/real/03__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/01__hugging_happy.mp4\n/kaggle/input/faceforensics/FF++/real/06__walking_down_indoor_hall_disgust.mp4\n/kaggle/input/faceforensics/FF++/real/11__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/15__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/02__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/01__walking_down_indoor_hall_disgust.mp4\n/kaggle/input/faceforensics/FF++/real/15__walking_down_indoor_hall_disgust.mp4\n/kaggle/input/faceforensics/FF++/real/08__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/01__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/05__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/01__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/05__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/02__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/01__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/04__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/05__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/02__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/05__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/11__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/14__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/07__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/03__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/01__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/13__secret_conversation.mp4\n/kaggle/input/faceforensics/FF++/real/10__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/04__secret_conversation.mp4\n/kaggle/input/faceforensics/FF++/real/02__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/10__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/13__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/07__secret_conversation.mp4\n/kaggle/input/faceforensics/FF++/real/03__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/10__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/02__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/14__hugging_happy.mp4\n/kaggle/input/faceforensics/FF++/real/02__meeting_serious.mp4\n/kaggle/input/faceforensics/FF++/real/10__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/04__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/11__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/01__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/12__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/13__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/04__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/07__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/04__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/14__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/05__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/02__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/14__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/06__walking_and_outside_surprised.mp4\n/kaggle/input/faceforensics/FF++/real/09__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/15__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/14__walking_and_outside_surprised.mp4\n/kaggle/input/faceforensics/FF++/real/01__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/10__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/02__hugging_happy.mp4\n/kaggle/input/faceforensics/FF++/real/04__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/12__hugging_happy.mp4\n/kaggle/input/faceforensics/FF++/real/15__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/06__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/03__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/03__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/07__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/08__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/05__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/03__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/12__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/10__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/11__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/10__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/03__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/15__hugging_happy.mp4\n/kaggle/input/faceforensics/FF++/real/01__secret_conversation.mp4\n/kaggle/input/faceforensics/FF++/real/10__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/15__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/03__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/15__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/02__secret_conversation.mp4\n/kaggle/input/faceforensics/FF++/real/10__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/05__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/04__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/13__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/08__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/15__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/07__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/12__secret_conversation.mp4\n/kaggle/input/faceforensics/FF++/real/03__walking_and_outside_surprised.mp4\n/kaggle/input/faceforensics/FF++/real/15__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/09__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/13__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/02__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/06__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/09__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/11__exit_phone_room.mp4\n/kaggle/input/faceforensics/FF++/real/07__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/09__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/15__talking_angry_couch.mp4\n/kaggle/input/faceforensics/FF++/real/08__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/13__walking_and_outside_surprised.mp4\n/kaggle/input/faceforensics/FF++/real/15__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/14__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/12__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/08__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/01__meeting_serious.mp4\n/kaggle/input/faceforensics/FF++/real/13__walking_down_street_outside_angry.mp4\n/kaggle/input/faceforensics/FF++/real/01__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/11__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/01__kitchen_still.mp4\n/kaggle/input/faceforensics/FF++/real/02__kitchen_pan.mp4\n/kaggle/input/faceforensics/FF++/real/03__meeting_serious.mp4\n/kaggle/input/faceforensics/FF++/real/06__walking_outside_cafe_disgusted.mp4\n/kaggle/input/faceforensics/FF++/real/15__walk_down_hall_angry.mp4\n/kaggle/input/faceforensics/FF++/real/03__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/03__talking_against_wall.mp4\n/kaggle/input/faceforensics/FF++/real/05__hugging_happy.mp4\n/kaggle/input/faceforensics/FF++/real/13__hugging_happy.mp4\n/kaggle/input/faceforensics/FF++/real/06__outside_talking_pan_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/02__outside_talking_still_laughing.mp4\n/kaggle/input/faceforensics/FF++/real/04__podium_speech_happy.mp4\n/kaggle/input/faceforensics/FF++/real/12__exit_phone_room.mp4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T07:00:57.244009Z","iopub.execute_input":"2025-03-24T07:00:57.244430Z","iopub.status.idle":"2025-03-24T07:01:29.431485Z","shell.execute_reply.started":"2025-03-24T07:00:57.244397Z","shell.execute_reply":"2025-03-24T07:01:29.429822Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'Deepfake-Detection-and-Generation'...\nfatal: unable to access 'https://github.com/K-a-y-D-e-e/Deepfake-Detection-and-Generation.git/': Could not resolve host: github.com\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Custom conv2d cnn\n","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport gc\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\n\n# ✅ Limit GPU Memory Usage for Kaggle\ndef limit_memory_usage():\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if gpus:\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n        except RuntimeError as e:\n            print(e)\n\ndef clear_memory():\n    gc.collect()\n    tf.keras.backend.clear_session()\n\n# ✅ Step 1: Get Dataset Paths (FIXED PATH)\ndef get_dataset_paths():\n    dataset_path = \"/kaggle/input/faceforensics/FF++\"  # ✅ Corrected dataset path\n\n    real_dir = os.path.join(dataset_path, \"real\")\n    fake_dir = os.path.join(dataset_path, \"fake\")\n\n    if not os.path.exists(real_dir) or not os.path.exists(fake_dir):\n        raise FileNotFoundError(f\"❌ Could not locate 'real' and 'fake' video folders in: {dataset_path}\")\n\n    print(f\"✅ Real Videos Path: {real_dir}\")\n    print(f\"✅ Fake Videos Path: {fake_dir}\")\n    return real_dir, fake_dir\n\n# ✅ Step 2: Extract Frames from Videos\ndef extract_frames(video_path, output_dir, max_frames=10, img_size=(96, 96)):\n    os.makedirs(output_dir, exist_ok=True)\n    cap = cv2.VideoCapture(video_path)\n\n    if not cap.isOpened():\n        print(f\"❌ Error opening video file: {video_path}\")\n        return []\n\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frames_to_extract = np.linspace(0, total_frames - 1, num=min(max_frames, total_frames), dtype=int)\n\n    saved_frames = []\n    frame_count = 0\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if frame_count in frames_to_extract:\n            try:\n                frame = cv2.resize(frame, img_size)\n                frame_path = os.path.join(output_dir, f\"frame_{frame_count}.jpg\")\n                cv2.imwrite(frame_path, frame)\n                saved_frames.append(frame_path)\n            except Exception as e:\n                print(f\"⚠️ Error processing frame {frame_count}: {e}\")\n\n        frame_count += 1\n\n    cap.release()\n    return saved_frames\n\n# ✅ Step 3: Process Dataset (UPDATED FOR FF++)\ndef process_dataset(real_dir, fake_dir, output_dir, max_videos=50):\n    os.makedirs(output_dir, exist_ok=True)\n    frame_data = []\n\n    def process_video_batch(video_dir, label):\n        batch_frames = []\n        for video_file in os.listdir(video_dir)[:max_videos]:\n            if video_file.endswith(('.mp4', '.avi')):\n                video_path = os.path.join(video_dir, video_file)\n                video_output_dir = os.path.join(output_dir, label, os.path.splitext(video_file)[0])\n                frames = extract_frames(video_path, video_output_dir)\n                for frame in frames:\n                    batch_frames.append({'path': frame, 'label': label})\n                clear_memory()\n        return batch_frames\n\n    frame_data += process_video_batch(real_dir, \"real\")\n    frame_data += process_video_batch(fake_dir, \"fake\")\n\n    return pd.DataFrame(frame_data)\n\n# ✅ Step 4: Define CNN Model\ndef create_cnn_model(input_shape=(96, 96, 3)):\n    model = Sequential([\n        Conv2D(16, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n        MaxPooling2D(pool_size=(2, 2)),\n        Conv2D(32, (3, 3), activation='relu', padding='same'),\n        MaxPooling2D(pool_size=(2, 2)),\n        Conv2D(64, (3, 3), activation='relu', padding='same'),\n        MaxPooling2D(pool_size=(2, 2)),\n        Flatten(),\n        Dense(128, activation='relu'),\n        Dropout(0.5),\n        Dense(1, activation='sigmoid')\n    ])\n    \n    model.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# ✅ Step 5: Main Function\ndef main():\n    print(\"🚀 Starting Deepfake Detection on FF++ Dataset...\")\n    limit_memory_usage()\n    real_dir, fake_dir = get_dataset_paths()\n    output_dir = \"/kaggle/working/frames\"\n\n    frame_df = process_dataset(real_dir, fake_dir, output_dir, max_videos=50)\n    datagen = ImageDataGenerator(rescale=1./255)\n\n    train_df, val_df = train_test_split(frame_df, test_size=0.2, stratify=frame_df['label'])\n\n    # ✅ Convert labels to strings\n    train_df['label'] = train_df['label'].astype(str)\n    val_df['label'] = val_df['label'].astype(str)\n\n    train_generator = datagen.flow_from_dataframe(train_df, x_col='path', y_col='label', target_size=(96, 96), batch_size=16, class_mode='binary')\n    val_generator = datagen.flow_from_dataframe(val_df, x_col='path', y_col='label', target_size=(96, 96), batch_size=16, class_mode='binary')\n\n    model = create_cnn_model()\n    model.fit(train_generator, validation_data=val_generator, epochs=10)\n\n# ✅ Run Main Function\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T17:11:49.107623Z","iopub.execute_input":"2025-03-23T17:11:49.107976Z","iopub.status.idle":"2025-03-23T17:19:19.550153Z","shell.execute_reply.started":"2025-03-23T17:11:49.107951Z","shell.execute_reply":"2025-03-23T17:19:19.549210Z"}},"outputs":[{"name":"stdout","text":"🚀 Starting Deepfake Detection on FF++ Dataset...\n✅ Real Videos Path: /kaggle/input/faceforensics/FF++/real\n✅ Fake Videos Path: /kaggle/input/faceforensics/FF++/fake\nFound 800 validated image filenames belonging to 2 classes.\nFound 200 validated image filenames belonging to 2 classes.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 94ms/step - accuracy: 0.5469 - loss: 0.7364 - val_accuracy: 0.6300 - val_loss: 0.6426\nEpoch 2/10\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - accuracy: 0.6334 - loss: 0.6289 - val_accuracy: 0.6650 - val_loss: 0.5702\nEpoch 3/10\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 85ms/step - accuracy: 0.7269 - loss: 0.5391 - val_accuracy: 0.7150 - val_loss: 0.5247\nEpoch 4/10\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - accuracy: 0.7358 - loss: 0.5121 - val_accuracy: 0.7300 - val_loss: 0.4928\nEpoch 5/10\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - accuracy: 0.7635 - loss: 0.4692 - val_accuracy: 0.7900 - val_loss: 0.4190\nEpoch 6/10\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 97ms/step - accuracy: 0.8201 - loss: 0.3713 - val_accuracy: 0.8450 - val_loss: 0.3523\nEpoch 7/10\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 84ms/step - accuracy: 0.8665 - loss: 0.3286 - val_accuracy: 0.8450 - val_loss: 0.3483\nEpoch 8/10\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - accuracy: 0.8878 - loss: 0.2876 - val_accuracy: 0.8700 - val_loss: 0.3251\nEpoch 9/10\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 85ms/step - accuracy: 0.8761 - loss: 0.3094 - val_accuracy: 0.8250 - val_loss: 0.3348\nEpoch 10/10\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - accuracy: 0.8908 - loss: 0.2445 - val_accuracy: 0.7900 - val_loss: 0.3499\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\ndef get_dataset_paths():\n    base_dir = \"/kaggle/input/faceforensics/FF++\"  # Change path if needed\n    real_dir = os.path.join(base_dir, \"real\")\n    fake_dir = os.path.join(base_dir, \"fake\")\n\n    if not os.path.exists(real_dir) or not os.path.exists(fake_dir):\n        raise FileNotFoundError(\"❌ Could not locate 'real' and 'fake' video folders!\")\n\n    print(f\"✅ Real Videos Path: {real_dir}\")\n    print(f\"✅ Fake Videos Path: {fake_dir}\")\n\n    return real_dir, fake_dir\ndef extract_frames(video_path, output_folder, max_frames=10):\n    os.makedirs(output_folder, exist_ok=True)\n\n    cap = cv2.VideoCapture(video_path)\n    frame_count = 0\n    success, frame = cap.read()\n\n    extracted_frames = []\n    \n    while success and frame_count < max_frames:\n        frame_path = os.path.join(output_folder, f\"{os.path.basename(video_path)}_frame{frame_count}.jpg\")\n        cv2.imwrite(frame_path, frame)\n        extracted_frames.append(frame_path)\n        frame_count += 1\n        success, frame = cap.read()\n\n    cap.release()\n    return extracted_frames\ndef prepare_dataset():\n    real_dir, fake_dir = get_dataset_paths()\n    output_dir = \"/kaggle/working/frames\"\n    data = []\n\n    for label, folder in [(\"real\", real_dir), (\"fake\", fake_dir)]:\n        for video in tqdm(os.listdir(folder), desc=f\"Processing {label} videos\"):\n            video_path = os.path.join(folder, video)\n            frame_output = os.path.join(output_dir, label, video.split('.')[0])\n\n            frame_paths = extract_frames(video_path, frame_output)\n\n            for frame_path in frame_paths:\n                if os.path.exists(frame_path):\n                    data.append((frame_path, label))\n\n    df = pd.DataFrame(data, columns=[\"path\", \"label\"])\n    \n    # 🔹 Fix missing values issue\n    df.dropna(inplace=True)\n    \n    print(f\"✅ Total Frames: {len(df)} (Real: {sum(df['label'] == 'real')}, Fake: {sum(df['label'] == 'fake')})\")\n\n    return df\ndef create_cnn_model():\n    model = Sequential([\n        Conv2D(32, (3, 3), activation='relu', input_shape=(96, 96, 3)),\n        MaxPooling2D(pool_size=(2, 2)),\n        Conv2D(64, (3, 3), activation='relu'),\n        MaxPooling2D(pool_size=(2, 2)),\n        Flatten(),\n        Dense(128, activation='relu'),\n        Dropout(0.5),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\ndef train_model(frame_df):\n    datagen = ImageDataGenerator(\n        rescale=1./255,\n        rotation_range=15,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        horizontal_flip=True,\n        zoom_range=0.2\n    )\n\n    # 🔹 Fix missing labels issue\n    frame_df.dropna(subset=[\"label\"], inplace=True)\n\n    # 🔹 Convert labels to string (\"real\", \"fake\")\n    frame_df[\"label\"] = frame_df[\"label\"].astype(str)\n\n    train_df, val_df = train_test_split(frame_df, test_size=0.2, stratify=frame_df[\"label\"])\n\n    train_generator = datagen.flow_from_dataframe(\n        train_df, x_col=\"path\", y_col=\"label\",\n        target_size=(96, 96), batch_size=16, class_mode=\"binary\"\n    )\n\n    val_generator = datagen.flow_from_dataframe(\n        val_df, x_col=\"path\", y_col=\"label\",\n        target_size=(96, 96), batch_size=16, class_mode=\"binary\"\n    )\n\n    # ✅ Define CNN Model\n    model = create_cnn_model()\n\n    # ✅ Add Early Stopping & Learning Rate Reduction\n    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n\n    # ✅ Train the model\n    model.fit(\n        train_generator,\n        validation_data=val_generator,\n        epochs=20,  # Increased for better convergence\n        callbacks=[early_stopping, reduce_lr]\n    )\n\n    return model\nif __name__ == \"__main__\":\n    print(\"🚀 Starting Deepfake Detection on FaceForensics++ Dataset...\")\n    \n    # ✅ Prepare dataset\n    frame_df = prepare_dataset()\n    \n    # ✅ Train model\n    model = train_model(frame_df)\n    \n    # ✅ Save trained model\n    model.save(\"/kaggle/working/deepfake_detector.h5\")\n    print(\"✅ Model saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T18:51:31.095286Z","iopub.execute_input":"2025-03-23T18:51:31.095766Z","iopub.status.idle":"2025-03-23T19:17:01.411171Z","shell.execute_reply.started":"2025-03-23T18:51:31.095731Z","shell.execute_reply":"2025-03-23T19:17:01.410094Z"}},"outputs":[{"name":"stdout","text":"🚀 Starting Deepfake Detection on FaceForensics++ Dataset...\n✅ Real Videos Path: /kaggle/input/faceforensics/FF++/real\n✅ Fake Videos Path: /kaggle/input/faceforensics/FF++/fake\n","output_type":"stream"},{"name":"stderr","text":"Processing real videos: 100%|██████████| 200/200 [00:42<00:00,  4.70it/s]\nProcessing fake videos: 100%|██████████| 200/200 [00:42<00:00,  4.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"✅ Total Frames: 4000 (Real: 2000, Fake: 2000)\nFound 3200 validated image filenames belonging to 2 classes.\nFound 800 validated image filenames belonging to 2 classes.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 360ms/step - accuracy: 0.5179 - loss: 0.7286 - val_accuracy: 0.5188 - val_loss: 0.6787 - learning_rate: 0.0010\nEpoch 2/20\n\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 356ms/step - accuracy: 0.5703 - loss: 0.6770 - val_accuracy: 0.6050 - val_loss: 0.6637 - learning_rate: 0.0010\nEpoch 3/20\n\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 357ms/step - accuracy: 0.5641 - loss: 0.6684 - val_accuracy: 0.6012 - val_loss: 0.6454 - learning_rate: 0.0010\nEpoch 4/20\n\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 353ms/step - accuracy: 0.5852 - loss: 0.6615 - val_accuracy: 0.6087 - val_loss: 0.6407 - learning_rate: 0.0010\nEpoch 5/20\n\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 350ms/step - accuracy: 0.5914 - loss: 0.6520 - val_accuracy: 0.6350 - val_loss: 0.6246 - learning_rate: 0.0010\nEpoch 6/20\n\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 358ms/step - accuracy: 0.6213 - loss: 0.6338 - val_accuracy: 0.6087 - val_loss: 0.6409 - learning_rate: 0.0010\nEpoch 7/20\n\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 350ms/step - accuracy: 0.6350 - loss: 0.6207 - val_accuracy: 0.6625 - val_loss: 0.5998 - learning_rate: 0.0010\nEpoch 8/20\n\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 351ms/step - accuracy: 0.6145 - loss: 0.6201 - val_accuracy: 0.6538 - val_loss: 0.5821 - learning_rate: 0.0010\nEpoch 9/20\n\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 346ms/step - accuracy: 0.6531 - loss: 0.6031 - val_accuracy: 0.6950 - val_loss: 0.5608 - learning_rate: 0.0010\nEpoch 10/20\n\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 348ms/step - accuracy: 0.6730 - loss: 0.5839 - val_accuracy: 0.6850 - val_loss: 0.5562 - learning_rate: 0.0010\nEpoch 11/20\n\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 352ms/step - accuracy: 0.6897 - loss: 0.5520 - val_accuracy: 0.7100 - val_loss: 0.5429 - learning_rate: 0.0010\nEpoch 12/20\n\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 349ms/step - accuracy: 0.6811 - loss: 0.5709 - val_accuracy: 0.7200 - val_loss: 0.5385 - learning_rate: 0.0010\nEpoch 13/20\n\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 347ms/step - accuracy: 0.7101 - loss: 0.5448 - val_accuracy: 0.7425 - val_loss: 0.5170 - learning_rate: 0.0010\nEpoch 14/20\n\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 351ms/step - accuracy: 0.6991 - loss: 0.5381 - val_accuracy: 0.7200 - val_loss: 0.5041 - learning_rate: 0.0010\nEpoch 15/20\n\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 348ms/step - accuracy: 0.7266 - loss: 0.5155 - val_accuracy: 0.7437 - val_loss: 0.5054 - learning_rate: 0.0010\nEpoch 16/20\n\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 357ms/step - accuracy: 0.7305 - loss: 0.5184 - val_accuracy: 0.7337 - val_loss: 0.5035 - learning_rate: 0.0010\nEpoch 17/20\n\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 350ms/step - accuracy: 0.7293 - loss: 0.5151 - val_accuracy: 0.7113 - val_loss: 0.5274 - learning_rate: 0.0010\nEpoch 18/20\n\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 356ms/step - accuracy: 0.7283 - loss: 0.5206 - val_accuracy: 0.7337 - val_loss: 0.4976 - learning_rate: 0.0010\nEpoch 19/20\n\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 350ms/step - accuracy: 0.7507 - loss: 0.4940 - val_accuracy: 0.7500 - val_loss: 0.4736 - learning_rate: 0.0010\nEpoch 20/20\n\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 349ms/step - accuracy: 0.7405 - loss: 0.5012 - val_accuracy: 0.7763 - val_loss: 0.4578 - learning_rate: 0.0010\n✅ Model saved successfully!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# squeeze and Excitation attention mechanism + Custom CNN","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport gc\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, Input, Reshape, Multiply\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\n\n# ✅ Limit GPU Memory Usage (For Kaggle)\ndef limit_memory_usage():\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if gpus:\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n        except RuntimeError as e:\n            print(e)\n\ndef clear_memory():\n    gc.collect()\n    tf.keras.backend.clear_session()\n\n# ✅ Step 1: Get Dataset Paths (Fix Path Issues)\ndef get_dataset_paths():\n    dataset_path = \"/kaggle/input/faceforensics/FF++\"  # Ensure correct dataset path\n\n    real_dir = os.path.join(dataset_path, \"real\")\n    fake_dir = os.path.join(dataset_path, \"fake\")\n\n    if not os.path.exists(real_dir) or not os.path.exists(fake_dir):\n        raise FileNotFoundError(f\"❌ Could not locate 'real' and 'fake' video folders in: {dataset_path}\")\n\n    print(f\"✅ Real Videos Path: {real_dir}\")\n    print(f\"✅ Fake Videos Path: {fake_dir}\")\n    return real_dir, fake_dir\n\n# ✅ Step 2: Extract Frames from Videos\ndef extract_frames(video_path, output_dir, max_frames=10, img_size=(96, 96)):\n    os.makedirs(output_dir, exist_ok=True)\n    cap = cv2.VideoCapture(video_path)\n\n    if not cap.isOpened():\n        print(f\"❌ Error opening video file: {video_path}\")\n        return []\n\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frames_to_extract = np.linspace(0, total_frames - 1, num=min(max_frames, total_frames), dtype=int)\n\n    saved_frames = []\n    frame_count = 0\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if frame_count in frames_to_extract:\n            try:\n                frame = cv2.resize(frame, img_size)\n                frame_path = os.path.join(output_dir, f\"frame_{frame_count}.jpg\")\n                cv2.imwrite(frame_path, frame)\n                saved_frames.append(frame_path)\n            except Exception as e:\n                print(f\"⚠️ Error processing frame {frame_count}: {e}\")\n\n        frame_count += 1\n\n    cap.release()\n    return saved_frames\n\n# ✅ Step 3: Process Dataset\ndef process_dataset(real_dir, fake_dir, output_dir, max_videos=50):\n    os.makedirs(output_dir, exist_ok=True)\n    frame_data = []\n\n    def process_video_batch(video_dir, label):\n        batch_frames = []\n        for video_file in os.listdir(video_dir)[:max_videos]:\n            if video_file.endswith(('.mp4', '.avi')):\n                video_path = os.path.join(video_dir, video_file)\n                video_output_dir = os.path.join(output_dir, label, os.path.splitext(video_file)[0])\n                frames = extract_frames(video_path, video_output_dir)\n                for frame in frames:\n                    batch_frames.append({'path': frame, 'label': label})\n                clear_memory()\n        return batch_frames\n\n    frame_data += process_video_batch(real_dir, \"real\")\n    frame_data += process_video_batch(fake_dir, \"fake\")\n\n    return pd.DataFrame(frame_data)\n\n# ✅ SE Attention Block\ndef se_block(input_tensor, reduction_ratio=16):\n    \"\"\"Squeeze-and-Excitation Block\"\"\"\n    filters = input_tensor.shape[-1]\n    se = GlobalAveragePooling2D()(input_tensor)\n    se = Dense(filters // reduction_ratio, activation='relu')(se)\n    se = Dense(filters, activation='sigmoid')(se)\n    se = Reshape((1, 1, filters))(se)\n    return Multiply()([input_tensor, se])\n\n# ✅ Step 4: Define CNN Model with SE Blocks\ndef create_se_cnn_model(input_shape=(96, 96, 3)):\n    \"\"\"CNN Model with Squeeze-and-Excitation Attention\"\"\"\n    inputs = Input(shape=input_shape)\n\n    x = Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n    x = se_block(x)  # SE Block\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n    x = se_block(x)  # SE Block\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n    x = se_block(x)  # SE Block\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Flatten()(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    outputs = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs, outputs)\n    model.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# ✅ Step 5: Main Function\ndef main():\n    print(\"🚀 Starting Deepfake Detection with SE Attention on FF++ Dataset...\")\n    limit_memory_usage()\n    real_dir, fake_dir = get_dataset_paths()\n    output_dir = \"/kaggle/working/frames\"\n\n    frame_df = process_dataset(real_dir, fake_dir, output_dir, max_videos=50)\n    datagen = ImageDataGenerator(rescale=1./255)\n\n    # ✅ Check if dataset is loaded correctly\n    if frame_df.empty:\n        raise ValueError(\"❌ Frame dataset is empty! Check video extraction.\")\n\n    print(\"✅ Total frames extracted:\", len(frame_df))\n\n    train_df, val_df = train_test_split(frame_df, test_size=0.2, stratify=frame_df['label'])\n\n    # ✅ Convert labels to strings (Fix for ValueError)\n    train_df['label'] = train_df['label'].astype(str)\n    val_df['label'] = val_df['label'].astype(str)\n\n    train_generator = datagen.flow_from_dataframe(\n        train_df, x_col='path', y_col='label', target_size=(96, 96), batch_size=16, class_mode='binary'\n    )\n    val_generator = datagen.flow_from_dataframe(\n        val_df, x_col='path', y_col='label', target_size=(96, 96), batch_size=16, class_mode='binary'\n    )\n\n    model = create_se_cnn_model()\n    model.summary()\n\n    # ✅ Train the Model\n    history = model.fit(\n        train_generator,\n        validation_data=val_generator,\n        epochs=20\n    )\n\n    # ✅ Save Model\n    model.save(\"/kaggle/working/se_cnn_deepfake_model.h5\")\n    print(\"✅ Model saved successfully!\")\n\n# ✅ Run Main Function\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T06:08:47.505890Z","iopub.execute_input":"2025-03-24T06:08:47.506717Z","iopub.status.idle":"2025-03-24T06:18:28.571266Z","shell.execute_reply.started":"2025-03-24T06:08:47.506679Z","shell.execute_reply":"2025-03-24T06:18:28.569348Z"}},"outputs":[{"name":"stdout","text":"🚀 Starting Deepfake Detection with SE Attention on FF++ Dataset...\n✅ Real Videos Path: /kaggle/input/faceforensics/FF++/real\n✅ Fake Videos Path: /kaggle/input/faceforensics/FF++/fake\n✅ Total frames extracted: 1000\nFound 800 validated image filenames belonging to 2 classes.\nFound 200 validated image filenames belonging to 2 classes.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │            \u001b[38;5;34m448\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m17\u001b[0m │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m32\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape (\u001b[38;5;33mReshape\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply (\u001b[38;5;33mMultiply\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n│                           │                        │                │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │          \u001b[38;5;34m4,640\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │             \u001b[38;5;34m66\u001b[0m │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m96\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply_1 (\u001b[38;5;33mMultiply\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│                           │                        │                │ reshape_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ multiply_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │            \u001b[38;5;34m260\u001b[0m │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │            \u001b[38;5;34m320\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape_2 (\u001b[38;5;33mReshape\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply_2 (\u001b[38;5;33mMultiply\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│                           │                        │                │ reshape_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ multiply_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9216\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ max_pooling2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_6 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │      \u001b[38;5;34m1,179,776\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_7 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m129\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n│                           │                        │                │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│                           │                        │                │ reshape_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling2d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ reshape_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ multiply_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│                           │                        │                │ reshape_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9216</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,179,776</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,204,280\u001b[0m (4.59 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,204,280</span> (4.59 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,204,280\u001b[0m (4.59 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,204,280</span> (4.59 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 118ms/step - accuracy: 0.4810 - loss: 0.6977 - val_accuracy: 0.6150 - val_loss: 0.6882\nEpoch 2/20\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 111ms/step - accuracy: 0.6061 - loss: 0.6807 - val_accuracy: 0.6050 - val_loss: 0.6480\nEpoch 3/20\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 105ms/step - accuracy: 0.6549 - loss: 0.6332 - val_accuracy: 0.6100 - val_loss: 0.6483\nEpoch 4/20\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 113ms/step - accuracy: 0.6343 - loss: 0.6310 - val_accuracy: 0.6850 - val_loss: 0.5821\nEpoch 5/20\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 116ms/step - accuracy: 0.6760 - loss: 0.5702 - val_accuracy: 0.6800 - val_loss: 0.5465\nEpoch 6/20\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 108ms/step - accuracy: 0.7078 - loss: 0.5248 - val_accuracy: 0.7350 - val_loss: 0.5196\nEpoch 7/20\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 111ms/step - accuracy: 0.7814 - loss: 0.4534 - val_accuracy: 0.7850 - val_loss: 0.4619\nEpoch 8/20\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.8124 - loss: 0.4081 - val_accuracy: 0.7950 - val_loss: 0.4417\nEpoch 9/20\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 111ms/step - accuracy: 0.8337 - loss: 0.3732 - val_accuracy: 0.7800 - val_loss: 0.4367\nEpoch 10/20\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 115ms/step - accuracy: 0.8550 - loss: 0.3264 - val_accuracy: 0.8150 - val_loss: 0.4030\nEpoch 11/20\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 108ms/step - accuracy: 0.8747 - loss: 0.3033 - val_accuracy: 0.8150 - val_loss: 0.4172\nEpoch 12/20\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.8733 - loss: 0.2978 - val_accuracy: 0.7850 - val_loss: 0.4263\nEpoch 13/20\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 110ms/step - accuracy: 0.9008 - loss: 0.2477 - val_accuracy: 0.8050 - val_loss: 0.4007\nEpoch 14/20\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 109ms/step - accuracy: 0.8679 - loss: 0.2669 - val_accuracy: 0.8100 - val_loss: 0.3877\nEpoch 15/20\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 106ms/step - accuracy: 0.8956 - loss: 0.2391 - val_accuracy: 0.7950 - val_loss: 0.4336\nEpoch 16/20\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - accuracy: 0.8879 - loss: 0.2457 - val_accuracy: 0.8200 - val_loss: 0.4009\nEpoch 17/20\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 110ms/step - accuracy: 0.8871 - loss: 0.2444 - val_accuracy: 0.8150 - val_loss: 0.3893\nEpoch 18/20\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 109ms/step - accuracy: 0.8918 - loss: 0.2295 - val_accuracy: 0.7850 - val_loss: 0.4279\nEpoch 19/20\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 106ms/step - accuracy: 0.9144 - loss: 0.1978 - val_accuracy: 0.8150 - val_loss: 0.4056\nEpoch 20/20\n\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 114ms/step - accuracy: 0.8928 - loss: 0.2127 - val_accuracy: 0.8000 - val_loss: 0.4025\n✅ Model saved successfully!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# cnn + lstm based on Dynamic feature extraction","metadata":{}},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, \n                                     BatchNormalization, LSTM, TimeDistributed, GlobalAveragePooling2D, Reshape, multiply)\nfrom tensorflow.keras.models import Model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\n# ✅ Detect Dataset Paths (FaceForensics++)\ndef get_dataset_paths():\n    dataset_path = \"/kaggle/input/faceforensics/FF++\"\n\n    real_dir = os.path.join(dataset_path, \"real\")\n    fake_dir = os.path.join(dataset_path, \"fake\")\n\n    if not os.path.exists(real_dir) or not os.path.exists(fake_dir):\n        raise FileNotFoundError(f\"❌ Could not locate 'real' and 'fake' folders in: {dataset_path}\")\n\n    print(f\"✅ Real Videos Path: {real_dir}\")\n    print(f\"✅ Fake Videos Path: {fake_dir}\")\n    return real_dir, fake_dir\n\n# ✅ Load Haarcascade Models for Feature Extraction\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\neye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\nmouth_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_mcs_mouth.xml')\n\n# ✅ Function to Extract Facial Features (Face, Eyes, Mouth)\ndef extract_facial_regions(frame):\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n\n    if len(faces) == 0:\n        return None  \n\n    for (x, y, w, h) in faces:\n        face = frame[y:y+h, x:x+w]\n        face_gray = gray[y:y+h, x:x+w]\n\n        eyes = eye_cascade.detectMultiScale(face_gray)\n        eye_regions = [face[ey:ey+eh, ex:ex+ew] for (ex, ey, ew, eh) in eyes]\n\n        mouth_rects = mouth_cascade.detectMultiScale(face_gray, 1.5, 5)\n        mouth = face[my:my+mh, mx:mx+mw] if len(mouth_rects) > 0 else None\n\n        return face, eye_regions, mouth\n\n    return None  \n\n# ✅ Function to Extract Frames from Videos (Face, Eyes, Mouth)\ndef extract_frames(video_path, output_dir, max_frames=10, img_size=(96, 96), mouth_size=(64, 32)):\n    os.makedirs(output_dir, exist_ok=True)\n    cap = cv2.VideoCapture(video_path)\n\n    if not cap.isOpened():\n        print(f\"❌ Error opening video file: {video_path}\")\n        return []\n\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frames_to_extract = np.linspace(0, total_frames - 1, num=min(max_frames, total_frames), dtype=int)\n\n    saved_frames = []\n    frame_count = 0\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if frame_count in frames_to_extract:\n            try:\n                face, eye_regions, mouth = extract_facial_regions(frame)\n\n                if face is not None:\n                    face = cv2.resize(face, img_size)\n                    face_path = os.path.join(output_dir, f\"face_{frame_count}.jpg\")\n                    cv2.imwrite(face_path, face)\n                    saved_frames.append(face_path)\n\n                for idx, eye in enumerate(eye_regions):\n                    eye = cv2.resize(eye, (32, 32))\n                    eye_path = os.path.join(output_dir, f\"eye_{idx}_{frame_count}.jpg\")\n                    cv2.imwrite(eye_path, eye)\n                    saved_frames.append(eye_path)\n\n                if mouth is not None:\n                    mouth = cv2.resize(mouth, mouth_size)\n                    mouth_path = os.path.join(output_dir, f\"mouth_{frame_count}.jpg\")\n                    cv2.imwrite(mouth_path, mouth)\n                    saved_frames.append(mouth_path)\n\n            except Exception as e:\n                print(f\"⚠️ Error processing frame {frame_count}: {e}\")\n\n        frame_count += 1\n\n    cap.release()\n    return saved_frames\n\n# ✅ SE Attention Block\ndef squeeze_excite_block(input_tensor, ratio=16):\n    filters = input_tensor.shape[-1]\n    se = GlobalAveragePooling2D()(input_tensor)\n    se = Dense(filters // ratio, activation='relu')(se)\n    se = Dense(filters, activation='sigmoid')(se)\n    se = Reshape((1, 1, filters))(se)\n    return multiply([input_tensor, se])\n\n# ✅ Define CNN-LSTM Model\ndef create_cnn_lstm_model(input_shape=(10, 96, 96, 3)):\n    cnn_input = Input(shape=(96, 96, 3))\n    x = Conv2D(32, (3, 3), activation='relu', padding='same')(cnn_input)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    x = squeeze_excite_block(x)  \n\n    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    x = squeeze_excite_block(x)\n\n    x = Flatten()(x)\n\n    cnn_model = Model(cnn_input, x, name=\"CNN_FeatureExtractor\")\n\n    video_input = Input(shape=input_shape, name=\"video_input\")\n    time_distributed = TimeDistributed(cnn_model)(video_input)  \n    lstm_out = LSTM(64, return_sequences=False)(time_distributed)\n\n    dense = Dense(128, activation='relu')(lstm_out)\n    dense = Dropout(0.5)(dense)\n    output = Dense(1, activation='sigmoid')(dense)  \n\n    model = Model(inputs=video_input, outputs=output)\n\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n    return model\n\n# ✅ Train & Evaluate Model\ndef train_cnn_lstm_model():\n    real_dir, fake_dir = get_dataset_paths()\n\n    train_videos = np.random.rand(500, 10, 96, 96, 3)  # Simulated dataset (replace with actual extracted frames)\n    train_labels = np.random.randint(0, 2, size=(500,))  \n\n    val_videos = np.random.rand(100, 10, 96, 96, 3)\n    val_labels = np.random.randint(0, 2, size=(100,))\n\n    model = create_cnn_lstm_model()\n\n    history = model.fit(train_videos, train_labels, validation_data=(val_videos, val_labels), epochs=15, batch_size=16)\n\n    model.save(\"/kaggle/working/cnn_lstm_dynamic_deepfake_model.h5\")\n\n    predictions = model.predict(val_videos)\n    binary_preds = [1 if p >= 0.5 else 0 for p in predictions]\n\n    acc = accuracy_score(val_labels, binary_preds)\n    precision = precision_score(val_labels, binary_preds)\n    recall = recall_score(val_labels, binary_preds)\n    f1 = f1_score(val_labels, binary_preds)\n    cm = confusion_matrix(val_labels, binary_preds)\n\n    print(f\"🔹 Accuracy: {acc:.4f}\")\n    print(f\"🔹 Precision: {precision:.4f}\")\n    print(f\"🔹 Recall: {recall:.4f}\")\n    print(f\"🔹 F1 Score: {f1:.4f}\")\n\nif __name__ == \"__main__\":\n    train_cnn_lstm_model()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T16:48:02.091141Z","iopub.execute_input":"2025-03-24T16:48:02.091521Z","iopub.status.idle":"2025-03-24T17:03:31.213282Z","shell.execute_reply.started":"2025-03-24T16:48:02.091491Z","shell.execute_reply":"2025-03-24T17:03:31.212053Z"}},"outputs":[{"name":"stdout","text":"✅ Real Videos Path: /kaggle/input/faceforensics/FF++/real\n✅ Fake Videos Path: /kaggle/input/faceforensics/FF++/fake\nEpoch 1/15\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 2s/step - accuracy: 0.4734 - loss: 0.7384 - val_accuracy: 0.5100 - val_loss: 0.7415\nEpoch 2/15\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 2s/step - accuracy: 0.5584 - loss: 0.6857 - val_accuracy: 0.4900 - val_loss: 0.6985\nEpoch 3/15\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.4685 - loss: 0.7329 - val_accuracy: 0.5100 - val_loss: 0.6930\nEpoch 4/15\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - accuracy: 0.4906 - loss: 0.7110 - val_accuracy: 0.5100 - val_loss: 0.6930\nEpoch 5/15\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.5063 - loss: 0.7024 - val_accuracy: 0.5100 - val_loss: 0.6930\nEpoch 6/15\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.5259 - loss: 0.6973 - val_accuracy: 0.4900 - val_loss: 0.6945\nEpoch 7/15\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - accuracy: 0.5113 - loss: 0.7010 - val_accuracy: 0.4900 - val_loss: 0.6973\nEpoch 8/15\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - accuracy: 0.5470 - loss: 0.6878 - val_accuracy: 0.4900 - val_loss: 0.6935\nEpoch 9/15\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - accuracy: 0.4688 - loss: 0.7152 - val_accuracy: 0.4900 - val_loss: 0.6933\nEpoch 10/15\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - accuracy: 0.5240 - loss: 0.6925 - val_accuracy: 0.4900 - val_loss: 0.7064\nEpoch 11/15\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - accuracy: 0.4895 - loss: 0.7029 - val_accuracy: 0.4900 - val_loss: 0.6999\nEpoch 12/15\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.5214 - loss: 0.6996 - val_accuracy: 0.5100 - val_loss: 0.6931\nEpoch 13/15\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.4931 - loss: 0.7018 - val_accuracy: 0.4900 - val_loss: 0.6976\nEpoch 14/15\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - accuracy: 0.5191 - loss: 0.6970 - val_accuracy: 0.5100 - val_loss: 0.6931\nEpoch 15/15\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - accuracy: 0.4884 - loss: 0.7003 - val_accuracy: 0.4900 - val_loss: 0.6951\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 942ms/step\n🔹 Accuracy: 0.4900\n🔹 Precision: 0.4900\n🔹 Recall: 1.0000\n🔹 F1 Score: 0.6577\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# LSTM ","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\n\ndef get_dataset_paths():\n    dataset_path = \"/kaggle/input/faceforensics/FF++\"\n\n    real_dir = os.path.join(dataset_path, \"real\")\n    fake_dir = os.path.join(dataset_path, \"fake\")\n\n    if not os.path.exists(real_dir) or not os.path.exists(fake_dir):\n        raise FileNotFoundError(f\"❌ Could not locate 'real' and 'fake' folders in: {dataset_path}\")\n\n    print(f\"✅ Real Videos Path: {real_dir}\")\n    print(f\"✅ Fake Videos Path: {fake_dir}\")\n    return real_dir, fake_dir\n\ndef extract_frames(video_path, max_frames=10, img_size=(96, 96)):\n    \"\"\"Extracts frames from a video and returns them as a sequence.\"\"\"\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(f\"❌ Error opening video: {video_path}\")\n        return None\n\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    frames_to_extract = np.linspace(0, total_frames - 1, num=max_frames, dtype=int)\n    frame_list = []\n\n    for frame_idx in range(total_frames):\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if frame_idx in frames_to_extract:\n            frame = cv2.resize(frame, img_size)\n            frame = frame / 255.0  \n            frame_list.append(frame)\n\n    cap.release()\n    return np.array(frame_list)\n\ndef process_dataset(real_dir, fake_dir, max_videos=50):\n    \"\"\"Extracts frames from real and fake videos, saves as dataset.\"\"\"\n    video_sequences = []\n    labels = []\n\n    for video_file in os.listdir(real_dir)[:max_videos]:\n        if video_file.endswith('.mp4'):\n            video_path = os.path.join(real_dir, video_file)\n            frames = extract_frames(video_path)\n            if frames is not None:\n                video_sequences.append(frames)\n                labels.append(0)  \n\n    for video_file in os.listdir(fake_dir)[:max_videos]:\n        if video_file.endswith('.mp4'):\n            video_path = os.path.join(fake_dir, video_file)\n            frames = extract_frames(video_path)\n            if frames is not None:\n                video_sequences.append(frames)\n                labels.append(1)  \n\n    video_sequences = np.array(video_sequences)  \n    labels = np.array(labels)\n\n    np.save(\"/kaggle/working/train_videos.npy\", video_sequences)\n    np.save(\"/kaggle/working/train_labels.npy\", labels)\n\n    print(f\"✅ Dataset saved: {video_sequences.shape}, Labels: {labels.shape}\")\n\nif __name__ == \"__main__\":\n    real_dir, fake_dir = get_dataset_paths()\n    process_dataset(real_dir, fake_dir)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-25T06:18:20.047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, TimeDistributed, Flatten\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# ✅ Load Dataset\ntrain_videos = np.load(\"/kaggle/working/train_videos.npy\")  # Shape (num_samples, 10, 96, 96, 3)\ntrain_labels = np.load(\"/kaggle/working/train_labels.npy\")\n\n# ✅ Reshape Data for LSTM\nnum_samples, time_steps, height, width, channels = train_videos.shape\nfeature_dim = height * width * channels  \n\ntrain_videos = train_videos.reshape(num_samples, time_steps, feature_dim)  # Flatten each frame\ntrain_videos, val_videos, train_labels, val_labels = train_test_split(train_videos, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n\n# ✅ Define LSTM Model\ndef create_lstm_model(input_shape=(10, 27648)):  # 10 time steps, flattened 96x96x3 frames\n    model = Sequential([\n        LSTM(128, return_sequences=True, input_shape=input_shape),\n        Dropout(0.5),\n        LSTM(64, return_sequences=False),\n        Dropout(0.5),\n        Dense(32, activation='relu'),\n        Dense(1, activation='sigmoid')  \n    ])\n    \n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# ✅ Train and Evaluate Model\ndef train_lstm_model():\n    model = create_lstm_model()\n\n    history = model.fit(train_videos, train_labels, validation_data=(val_videos, val_labels), epochs=15, batch_size=16)\n\n    predictions = model.predict(val_videos)\n    binary_preds = [1 if p >= 0.5 else 0 for p in predictions]\n\n    acc = accuracy_score(val_labels, binary_preds)\n    precision = precision_score(val_labels, binary_preds)\n    recall = recall_score(val_labels, binary_preds)\n    f1 = f1_score(val_labels, binary_preds)\n    cm = confusion_matrix(val_labels, binary_preds)\n\n    print(f\"\\n🔹 Accuracy: {acc:.4f}\")\n    print(f\"🔹 Precision: {precision:.4f}\")\n    print(f\"🔹 Recall: {recall:.4f}\")\n    print(f\"🔹 F1 Score: {f1:.4f}\")\n\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.title(\"Confusion Matrix - LSTM Only\")\n    plt.show()\n\n# ✅ Run Experiment\nif __name__ == \"__main__\":\n    train_lstm_model()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T17:37:20.616750Z","iopub.execute_input":"2025-03-24T17:37:20.617245Z","iopub.status.idle":"2025-03-24T17:38:00.608322Z","shell.execute_reply.started":"2025-03-24T17:37:20.617210Z","shell.execute_reply":"2025-03-24T17:38:00.607181Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/15\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 553ms/step - accuracy: 0.4905 - loss: 0.7099 - val_accuracy: 0.5500 - val_loss: 0.6976\nEpoch 2/15\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 573ms/step - accuracy: 0.5910 - loss: 0.6868 - val_accuracy: 0.5000 - val_loss: 0.6978\nEpoch 3/15\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 447ms/step - accuracy: 0.5639 - loss: 0.6922 - val_accuracy: 0.5000 - val_loss: 0.7102\nEpoch 4/15\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 418ms/step - accuracy: 0.5592 - loss: 0.6793 - val_accuracy: 0.5000 - val_loss: 0.7164\nEpoch 5/15\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 412ms/step - accuracy: 0.4856 - loss: 0.7099 - val_accuracy: 0.5000 - val_loss: 0.7104\nEpoch 6/15\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 530ms/step - accuracy: 0.4932 - loss: 0.6996 - val_accuracy: 0.5000 - val_loss: 0.6992\nEpoch 7/15\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 437ms/step - accuracy: 0.4024 - loss: 0.7181 - val_accuracy: 0.5000 - val_loss: 0.7000\nEpoch 8/15\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 412ms/step - accuracy: 0.5352 - loss: 0.6888 - val_accuracy: 0.5000 - val_loss: 0.7035\nEpoch 9/15\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 445ms/step - accuracy: 0.5339 - loss: 0.7113 - val_accuracy: 0.5000 - val_loss: 0.7029\nEpoch 10/15\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 415ms/step - accuracy: 0.3929 - loss: 0.7267 - val_accuracy: 0.5000 - val_loss: 0.7001\nEpoch 11/15\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 436ms/step - accuracy: 0.5505 - loss: 0.6880 - val_accuracy: 0.5000 - val_loss: 0.7103\nEpoch 12/15\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 412ms/step - accuracy: 0.5097 - loss: 0.6958 - val_accuracy: 0.5000 - val_loss: 0.7061\nEpoch 13/15\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 445ms/step - accuracy: 0.4783 - loss: 0.6838 - val_accuracy: 0.5000 - val_loss: 0.6998\nEpoch 14/15\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 439ms/step - accuracy: 0.4769 - loss: 0.7034 - val_accuracy: 0.5000 - val_loss: 0.6927\nEpoch 15/15\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 429ms/step - accuracy: 0.5125 - loss: 0.7012 - val_accuracy: 0.5000 - val_loss: 0.6951\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409ms/step\n\n🔹 Accuracy: 0.5000\n🔹 Precision: 0.5000\n🔹 Recall: 1.0000\n🔹 F1 Score: 0.6667\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x500 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"# Bidirectional LSTM\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, BatchNormalization, TimeDistributed, Flatten\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# ✅ Load Dataset\ntrain_videos = np.load(\"/kaggle/working/train_videos.npy\")  # Shape (num_samples, 10, 96, 96, 3)\ntrain_labels = np.load(\"/kaggle/working/train_labels.npy\")\n\n# ✅ Normalize Data (0 to 1)\ntrain_videos = train_videos / 255.0  \n\n# ✅ Reshape Data for LSTM\nnum_samples, time_steps, height, width, channels = train_videos.shape\nfeature_dim = height * width * channels  \n\ntrain_videos = train_videos.reshape(num_samples, time_steps, feature_dim)  # Flatten each frame\ntrain_videos, val_videos, train_labels, val_labels = train_test_split(train_videos, train_labels, test_size=0.2, random_state=42, stratify=train_labels)\n\n# ✅ Define Bi-LSTM Model\ndef create_lstm_model(input_shape=(5, 27648)):  # 10 time steps, flattened 96x96x3 frames\n    model = Sequential([\n        Bidirectional(LSTM(256, return_sequences=True, input_shape=input_shape)),  # 🔥 Use Bi-LSTM\n        BatchNormalization(),\n        Dropout(0.3),\n\n        Bidirectional(LSTM(128, return_sequences=False)),  # 🔥 Another Bi-LSTM layer\n        BatchNormalization(),\n        Dropout(0.3),\n\n        Dense(64, activation='relu'),\n        Dropout(0.3),\n\n        Dense(1, activation='sigmoid')  \n    ])\n    \n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# ✅ Train and Evaluate Model\ndef train_lstm_model():\n    model = create_lstm_model()\n\n    history = model.fit(\n        train_videos, train_labels,\n        validation_data=(val_videos, val_labels),\n        epochs=25, batch_size=10\n    )\n\n    predictions = model.predict(val_videos)\n    binary_preds = [1 if p >= 0.5 else 0 for p in predictions]\n\n    acc = accuracy_score(val_labels, binary_preds)\n    precision = precision_score(val_labels, binary_preds)\n    recall = recall_score(val_labels, binary_preds)\n    f1 = f1_score(val_labels, binary_preds)\n    cm = confusion_matrix(val_labels, binary_preds)\n\n    print(f\"\\n🔹 Accuracy: {acc:.4f}\")\n    print(f\"🔹 Precision: {precision:.4f}\")\n    print(f\"🔹 Recall: {recall:.4f}\")\n    print(f\"🔹 F1 Score: {f1:.4f}\")\n\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.title(\"Confusion Matrix - Bi-LSTM\")\n    plt.show()\n\n# ✅ Run Experiment\nif __name__ == \"__main__\":\n    train_lstm_model()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T17:53:31.825597Z","iopub.execute_input":"2025-03-24T17:53:31.826017Z","iopub.status.idle":"2025-03-24T17:59:02.010769Z","shell.execute_reply.started":"2025-03-24T17:53:31.825982Z","shell.execute_reply":"2025-03-24T17:59:02.009477Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step - accuracy: 0.6905 - loss: 0.8889 - val_accuracy: 0.6000 - val_loss: 0.6946\nEpoch 2/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.6427 - loss: 0.7774 - val_accuracy: 0.4500 - val_loss: 0.6941\nEpoch 3/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.5434 - loss: 0.9289 - val_accuracy: 0.5500 - val_loss: 0.6932\nEpoch 4/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.6582 - loss: 0.7379 - val_accuracy: 0.4500 - val_loss: 0.6926\nEpoch 5/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.5769 - loss: 0.6961 - val_accuracy: 0.5000 - val_loss: 0.7071\nEpoch 6/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.6269 - loss: 0.7512 - val_accuracy: 0.5000 - val_loss: 0.7261\nEpoch 7/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.6858 - loss: 0.6461 - val_accuracy: 0.5500 - val_loss: 0.7009\nEpoch 8/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.6375 - loss: 0.7326 - val_accuracy: 0.4500 - val_loss: 0.7002\nEpoch 9/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.7986 - loss: 0.4897 - val_accuracy: 0.5000 - val_loss: 0.7059\nEpoch 10/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.6486 - loss: 0.6346 - val_accuracy: 0.5000 - val_loss: 0.7748\nEpoch 11/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.6526 - loss: 0.5860 - val_accuracy: 0.5000 - val_loss: 0.7424\nEpoch 12/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.7634 - loss: 0.4886 - val_accuracy: 0.4000 - val_loss: 0.7173\nEpoch 13/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7842 - loss: 0.4871 - val_accuracy: 0.4000 - val_loss: 0.7279\nEpoch 14/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7316 - loss: 0.6436 - val_accuracy: 0.5000 - val_loss: 0.7565\nEpoch 15/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7465 - loss: 0.4707 - val_accuracy: 0.5000 - val_loss: 0.7656\nEpoch 16/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7925 - loss: 0.4466 - val_accuracy: 0.5000 - val_loss: 0.7688\nEpoch 17/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7953 - loss: 0.4782 - val_accuracy: 0.5000 - val_loss: 0.7845\nEpoch 18/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.8179 - loss: 0.3856 - val_accuracy: 0.5500 - val_loss: 0.8070\nEpoch 19/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.7670 - loss: 0.4801 - val_accuracy: 0.6500 - val_loss: 0.7788\nEpoch 20/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.8889 - loss: 0.3177 - val_accuracy: 0.5000 - val_loss: 0.9016\nEpoch 21/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.8781 - loss: 0.2450 - val_accuracy: 0.5000 - val_loss: 0.9500\nEpoch 22/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.8873 - loss: 0.2619 - val_accuracy: 0.5000 - val_loss: 1.2721\nEpoch 23/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.9101 - loss: 0.2604 - val_accuracy: 0.5000 - val_loss: 1.0600\nEpoch 24/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.8920 - loss: 0.2466 - val_accuracy: 0.5000 - val_loss: 1.1201\nEpoch 25/25\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.9339 - loss: 0.1466 - val_accuracy: 0.5000 - val_loss: 1.2547\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 996ms/step\n\n🔹 Accuracy: 0.5000\n🔹 Precision: 0.5000\n🔹 Recall: 1.0000\n🔹 F1 Score: 0.6667\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x500 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    classification_report, \n    confusion_matrix, \n    roc_curve, \n    auc, \n    precision_recall_curve\n)\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (\n    Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, \n    Dense, Dropout, LSTM, Bidirectional, TimeDistributed, \n    BatchNormalization, concatenate\n)\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.callbacks import (\n    ReduceLROnPlateau, \n    EarlyStopping, \n    ModelCheckpoint\n)\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nclass VideoDataGenerator(Sequence):\n    \"\"\"Advanced data generator for video sequences with augmentation\"\"\"\n    def __init__(self, dataframe, batch_size=32, shuffle=True, augment=True):\n        self.dataframe = dataframe\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.augment = augment\n        self.indexes = np.arange(len(self.dataframe))\n        \n        # Image augmentation\n        self.image_datagen = ImageDataGenerator(\n            rotation_range=20,\n            width_shift_range=0.2,\n            height_shift_range=0.2,\n            horizontal_flip=True,\n            zoom_range=0.2,\n            shear_range=0.2\n        ) if augment else None\n        \n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n    \n    def __len__(self):\n        return int(np.ceil(len(self.dataframe) / self.batch_size))\n    \n    def __getitem__(self, idx):\n        batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_df = self.dataframe.iloc[batch_indexes]\n        \n        X_batch = []\n        y_batch = []\n        \n        for _, row in batch_df.iterrows():\n            frames = self.extract_frames(row['video_path'])\n            if frames is not None:\n                # Augment frames if enabled\n                if self.augment and self.image_datagen:\n                    augmented_frames = []\n                    for frame in frames:\n                        # Apply augmentation to each frame\n                        frame = self.image_datagen.random_transform(frame)\n                        augmented_frames.append(frame)\n                    frames = np.array(augmented_frames)\n                \n                X_batch.append(frames)\n                y_batch.append(row['label'])\n        \n        return np.array(X_batch), np.array(y_batch)\n    \n    def on_epoch_end(self):\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n    \n    @staticmethod\n    def extract_frames(video_path, sequence_length=10, img_size=(96, 96)):\n        \"\"\"\n        Extract frames from a video with intelligent sampling and preprocessing\n        \n        Args:\n            video_path (str): Path to the video file\n            sequence_length (int): Number of frames to extract\n            img_size (tuple): Desired frame resize dimensions\n        \n        Returns:\n            np.array: Normalized video frames\n        \"\"\"\n        try:\n            cap = cv2.VideoCapture(video_path)\n            if not cap.isOpened():\n                print(f\"Error opening video file: {video_path}\")\n                return None\n\n            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n            \n            # Intelligent frame sampling\n            frame_indices = np.linspace(0, total_frames - 1, sequence_length).astype(int)\n\n            frames = []\n            for frame_count in range(total_frames):\n                ret, frame = cap.read()\n                if not ret:\n                    break\n                \n                if frame_count in frame_indices:\n                    # Resize and preprocess frame\n                    frame = cv2.resize(frame, img_size)\n                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                    \n                    # Normalize and convert to float\n                    frame = frame.astype(np.float32) / 255.0\n                    \n                    frames.append(frame)\n\n            cap.release()\n\n            # Handle videos shorter than sequence length\n            if len(frames) < sequence_length:\n                # Pad with last frame\n                frames += [frames[-1]] * (sequence_length - len(frames))\n            \n            return np.array(frames)\n        \n        except Exception as e:\n            print(f\"Error processing {video_path}: {e}\")\n            return None\n\ndef create_advanced_cnn_lstm_model(input_shape=(10, 96, 96, 3)):\n    \"\"\"\n    Create an advanced CNN-LSTM hybrid model for deepfake detection\n    \n    Args:\n        input_shape (tuple): Shape of input video frames\n    \n    Returns:\n        tf.keras.Model: Compiled deepfake detection model\n    \"\"\"\n    inputs = Input(shape=input_shape)\n    \n    # Spatial feature extraction\n    x = TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'))(inputs)\n    x = TimeDistributed(BatchNormalization())(x)\n    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n    \n    x = TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same'))(x)\n    x = TimeDistributed(BatchNormalization())(x)\n    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n    \n    x = TimeDistributed(Conv2D(128, (3, 3), activation='relu', padding='same'))(x)\n    x = TimeDistributed(BatchNormalization())(x)\n    x = TimeDistributed(GlobalAveragePooling2D())(x)\n    \n    # Temporal feature extraction\n    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3))(x)\n    x = Bidirectional(LSTM(64, dropout=0.3))(x)\n    \n    # Classification head\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    \n    outputs = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs=inputs, outputs=outputs)\n    \n    # Advanced optimizer with custom learning rate\n    optimizer = Adam(\n        learning_rate=1e-4, \n        beta_1=0.9, \n        beta_2=0.999, \n        epsilon=1e-8\n    )\n    \n    model.compile(\n        optimizer=optimizer, \n        loss='binary_crossentropy', \n        metrics=['accuracy', 'precision', 'recall']\n    )\n    \n    return model\n\ndef find_dataset_paths(base_paths=None):\n    \"\"\"\n    Dynamically find dataset paths\n    \n    Args:\n        base_paths (list): Optional list of paths to search\n    \n    Returns:\n        tuple: Paths to real and fake video directories\n    \"\"\"\n    if base_paths is None:\n        base_paths = [\n            '/kaggle/input/faceforensics',\n            '/kaggle/input/faceforensics/FF++',\n            '/kaggle/input',\n            '/content',\n            '.'\n        ]\n    \n    for base_path in base_paths:\n        # Check multiple possible dataset structures\n        dataset_paths = [\n            os.path.join(base_path, 'real'),\n            os.path.join(base_path, 'fake'),\n            os.path.join(base_path, 'FF++', 'real'),\n            os.path.join(base_path, 'FF++', 'fake')\n        ]\n        \n        for real_dir in dataset_paths:\n            for fake_dir in dataset_paths:\n                if real_dir != fake_dir and os.path.exists(real_dir) and os.path.exists(fake_dir):\n                    print(f\"🎉 Dataset found!\")\n                    print(f\"   Real videos: {real_dir}\")\n                    print(f\"   Fake videos: {fake_dir}\")\n                    return real_dir, fake_dir\n    \n    raise FileNotFoundError(\"No dataset found. Please provide valid paths.\")\n\ndef process_dataset(real_dir, fake_dir, max_videos=None):\n    \"\"\"\n    Process video dataset\n    \n    Args:\n        real_dir (str): Directory containing real videos\n        fake_dir (str): Directory containing fake videos\n        max_videos (int, optional): Maximum number of videos to process\n    \n    Returns:\n        pd.DataFrame: Processed dataset\n    \"\"\"\n    def process_directory(directory, label):\n        videos = []\n        for video_file in os.listdir(directory)[:max_videos]:\n            video_path = os.path.join(directory, video_file)\n            videos.append({'video_path': video_path, 'label': label})\n        return videos\n    \n    # Process real and fake videos\n    real_videos = process_directory(real_dir, 0)\n    fake_videos = process_directory(fake_dir, 1)\n    \n    # Combine and shuffle\n    df = pd.DataFrame(real_videos + fake_videos)\n    df = df.sample(frac=1).reset_index(drop=True)\n    \n    return df\n\ndef plot_training_history(history):\n    \"\"\"\n    Visualize model training history\n    \n    Args:\n        history (tf.keras.callbacks.History): Training history object\n    \"\"\"\n    plt.figure(figsize=(15, 5))\n    \n    # Loss subplot\n    plt.subplot(1, 3, 1)\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    # Accuracy subplot\n    plt.subplot(1, 3, 2)\n    plt.plot(history.history['accuracy'], label='Training Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title('Model Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    # Precision and Recall\n    plt.subplot(1, 3, 3)\n    plt.plot(history.history['precision'], label='Training Precision')\n    plt.plot(history.history['val_precision'], label='Validation Precision')\n    plt.plot(history.history['recall'], label='Training Recall')\n    plt.plot(history.history['val_recall'], label='Validation Recall')\n    plt.title('Precision & Recall')\n    plt.xlabel('Epoch')\n    plt.ylabel('Score')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n\ndef evaluate_model(model, val_generator):\n    \"\"\"\n    Comprehensive model evaluation\n    \n    Args:\n        model (tf.keras.Model): Trained model\n        val_generator (VideoDataGenerator): Validation data generator\n    \n    Returns:\n        dict: Evaluation metrics\n    \"\"\"\n    # Collect true labels and predictions\n    y_true = []\n    y_pred_proba = []\n    \n    for X, y in val_generator:\n        predictions = model.predict(X)\n        y_true.extend(y)\n        y_pred_proba.extend(predictions)\n    \n    # Convert probabilities to binary predictions\n    y_pred = (np.array(y_pred_proba) > 0.5).astype(int)\n    \n    # Classification Report\n    print(\"\\n📊 Detailed Classification Report:\")\n    report = classification_report(y_true, y_pred, target_names=['Real', 'Fake'])\n    print(report)\n    \n    # Confusion Matrix\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.show()\n    \n    # ROC Curve\n    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n    roc_auc = auc(fpr, tpr)\n    \n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n    return {\n        'classification_report': report,\n        'confusion_matrix': cm,\n        'roc_auc': roc_auc\n    }\n\ndef train_deepfake_model(\n    test_size=0.2, \n    max_videos=None, \n    epochs=50, \n    batch_size=32\n):\n    \"\"\"\n    Train deepfake detection model\n    \n    Args:\n        test_size (float): Proportion of dataset for validation\n        max_videos (int): Maximum number of videos to process\n        epochs (int): Number of training epochs\n        batch_size (int): Training batch size\n    \n    Returns:\n        tuple: Trained model, training history, evaluation metrics\n    \"\"\"\n    # Find dataset paths\n    real_dir, fake_dir = find_dataset_paths()\n    \n    # Process dataset\n    print(\"🚀 Processing Dataset...\")\n    df = process_dataset(real_dir, fake_dir, max_videos)\n    \n    # Split dataset\n    train_df, val_df = train_test_split(\n        df, \n        test_size=test_size, \n        stratify=df['label'], \n        random_state=42\n    )\n    \n    # Create data generators\n    train_generator = VideoDataGenerator(\n        train_df, \n        batch_size=batch_size, \n        shuffle=True, \n        augment=True\n    )\n    val_generator = VideoDataGenerator(\n        val_df, \n        batch_size=batch_size, \n        shuffle=False, \n        augment=False\n    )\n    \n    # Model creation\n    model = create_advanced_cnn_lstm_model()\n    \n    # Callbacks\n    early_stopping = EarlyStopping(\n        monitor='val_loss', \n        patience=10, \n        restore_best_weights=True\n    )\n    \n    lr_reducer = ReduceLROnPlateau(\n        monitor='val_loss', \n        factor=0.5, \n        patience=5, \n        min_lr=1e-6\n    )\n    \n    model_checkpoint = ModelCheckpoint(\n        'best_model.keras',  # Changed file extension to .keras\n        save_best_only=True, \n        monitor='val_accuracy'\n    )\n    \n    # Training\n    print(\"🚀 Training Model...\")\n    history = model.fit(\n        train_generator, \n        validation_data=val_generator, \n        epochs=epochs,\n        callbacks=[early_stopping, lr_reducer, model_checkpoint]\n    )\n    \n    # Visualization\n    plot_training_history(history)\n    \n    # Evaluation\n    evaluation_metrics = evaluate_model(model, val_generator)\n    \n    return model, history, evaluation_metrics\n\ndef predict_video(model, video_path):\n    \"\"\"\n    Predict whether a single video is real or fake\n    \n    Args:\n        model (tf.keras.Model): Trained deepfake detection model\n        video_path (str): Path to video file to predict\n    \n    Returns:\n        float: Probability of being a fake video\n    \"\"\"\n    # Extract frames\n    frames = VideoDataGenerator.extract_frames(video_path)\n    \n    if frames is not None:\n        # Add batch dimension\n        frames = np.expand_dims(frames, axis=0)\n        prediction = model.predict(frames)[0][0]\n        \n        # Interpret prediction\n        label = \"Fake\" if prediction > 0.5 else \"Real\"\n        confidence = prediction if prediction > 0.5 else 1 - prediction\n        \n        print(f\"🎬 Video Analysis:\")\n        print(f\"Predicted Label: {label}\")\n        print(f\"Confidence: {confidence:.2%}\")\n        \n        return prediction\n    \n    print(\"Could not process video.\")\n    return None\n\ndef main():\n    # Train the model\n    model, history, metrics = train_deepfake_model()\n    \n    # Optional: Predict a specific video\n    # Replace with an actual video path from your dataset\n    # sample_video_path = '/path/to/your/video.mp4'\n    # predict_video(model, sample_video_path)\n    \n    return model, history, metrics\nif __name__ == \"__main__\":\n    main()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T14:08:06.694367Z","iopub.execute_input":"2025-03-27T14:08:06.694692Z","execution_failed":"2025-03-27T18:40:16.237Z"}},"outputs":[{"name":"stdout","text":"🎉 Dataset found!\n   Real videos: /kaggle/input/faceforensics/FF++/real\n   Fake videos: /kaggle/input/faceforensics/FF++/fake\n🚀 Processing Dataset...\n🚀 Training Model...\nEpoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1436s\u001b[0m 69s/step - accuracy: 0.5692 - loss: 0.6995 - precision: 0.5950 - recall: 0.3576 - val_accuracy: 0.5000 - val_loss: 0.6971 - val_precision: 0.5000 - val_recall: 0.0500 - learning_rate: 1.0000e-04\nEpoch 2/50\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1177s\u001b[0m 48s/step - accuracy: 0.5157 - loss: 0.6897 - precision: 0.4998 - recall: 0.4106 - val_accuracy: 0.5375 - val_loss: 0.6921 - val_precision: 0.7143 - val_recall: 0.1250 - learning_rate: 1.0000e-04\nEpoch 3/50\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1173s\u001b[0m 49s/step - accuracy: 0.4633 - loss: 0.7017 - precision: 0.4275 - recall: 0.3193 - val_accuracy: 0.4875 - val_loss: 0.7043 - val_precision: 0.4000 - val_recall: 0.0500 - learning_rate: 1.0000e-04\nEpoch 4/50\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1212s\u001b[0m 49s/step - accuracy: 0.5080 - loss: 0.6979 - precision: 0.5420 - recall: 0.3764 - val_accuracy: 0.4750 - val_loss: 0.6935 - val_precision: 0.3750 - val_recall: 0.0750 - learning_rate: 1.0000e-04\nEpoch 5/50\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1236s\u001b[0m 52s/step - accuracy: 0.5214 - loss: 0.6911 - precision: 0.5205 - recall: 0.3692 - val_accuracy: 0.5125 - val_loss: 0.6971 - val_precision: 0.5455 - val_recall: 0.1500 - learning_rate: 1.0000e-04\nEpoch 6/50\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1241s\u001b[0m 50s/step - accuracy: 0.5561 - loss: 0.6933 - precision: 0.5774 - recall: 0.4512 - val_accuracy: 0.5500 - val_loss: 0.6958 - val_precision: 0.6111 - val_recall: 0.2750 - learning_rate: 1.0000e-04\nEpoch 7/50\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1225s\u001b[0m 50s/step - accuracy: 0.4753 - loss: 0.7020 - precision: 0.4546 - recall: 0.3865 - val_accuracy: 0.5000 - val_loss: 0.7012 - val_precision: 0.5000 - val_recall: 0.2750 - learning_rate: 1.0000e-04\nEpoch 8/50\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1235s\u001b[0m 49s/step - accuracy: 0.5033 - loss: 0.6936 - precision: 0.5429 - recall: 0.4111 - val_accuracy: 0.4250 - val_loss: 0.7021 - val_precision: 0.3636 - val_recall: 0.2000 - learning_rate: 5.0000e-05\nEpoch 9/50\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1232s\u001b[0m 47s/step - accuracy: 0.4707 - loss: 0.6924 - precision: 0.4579 - recall: 0.3333 - val_accuracy: 0.4375 - val_loss: 0.7031 - val_precision: 0.4000 - val_recall: 0.2500 - learning_rate: 5.0000e-05\nEpoch 10/50\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24s/step - accuracy: 0.5170 - loss: 0.6847 - precision: 0.5502 - recall: 0.4034 ","output_type":"stream"}],"execution_count":null}]}